{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c7fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import pygeohash as gh\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "geohash_prec = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e589596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/homebrew/Cellar/spark-3.3.2-bin-hadoop3\") # 指明SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4287b6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/01 13:52:29 WARN Utils: Your hostname, xhjdiedemaikebuke.local resolves to a loopback address: 127.0.0.1; using 10.176.49.240 instead (on interface en0)\n",
      "23/05/01 13:52:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/01 13:52:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\")\\\n",
    ".config(\"spark.some.config.option\", \"some-value\")\\\n",
    ".config(\"spark.debug.maxToStringFields\", \"100\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba676931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weather:\n",
    "    date = ''\n",
    "    temp = 0.0\n",
    "    windchill = 0.0\n",
    "    humid = 0.0\n",
    "    pressure= 0.0\n",
    "    visib = 0.0\n",
    "    windspeed = 0.0\n",
    "    winddir = ''\n",
    "    precipitation = 0.0\n",
    "    events = ''\n",
    "    condition = ''\n",
    "    \n",
    "    def __init__(self, date, temp, windchill, humid, pressure, visib, windspeed, winddir, \n",
    "                 precipitation, events, condition, zone):\n",
    "        self.date = datetime.strptime(date, '%Y-%m-%d %I:%M:%S %p')\n",
    "        self.date = self.date.replace(tzinfo=pytz.timezone(zone))\n",
    "        self.temp = float(temp)\n",
    "        self.windchill = float(windchill)\n",
    "        self.humid = float(humid)\n",
    "        self.pressure = float(pressure)\n",
    "        self.visib = float(visib)\n",
    "        self.windspeed = float(windspeed)\n",
    "        self.winddir = winddir\n",
    "        self.precipitation = float(precipitation)\n",
    "        self.events = events\n",
    "        self.condition = condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8ee80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:====================================================>    (57 + 5) / 62]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used to read the file: 17.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:=====================================================>   (58 + 4) / 62]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Get the current time before reading the file\n",
    "start_time = time.time()\n",
    "\n",
    "acc_df = spark.read.option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .csv(\"TrafficEvents_Aug16_Dec20_Publish.csv\")\n",
    "\n",
    "acc_df.head()\n",
    "\n",
    "# Get the current time after reading the file\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time in seconds\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time used to read the file: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be63d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, date_format\n",
    "acc_df = acc_df.withColumn(\"StartTime(UTC)\",\n",
    "    date_format(\n",
    "        to_timestamp(acc_df[\"StartTime(UTC)\"], \"yyyy-MM-dd'T'HH:mm:ss\"), \n",
    "        \"yyyy-MM-dd'T'HH:mm:ss\"\n",
    "    ).alias('timestamp_value'))\n",
    "\n",
    "acc_df = acc_df.withColumn(\"EndTime(UTC)\",\n",
    "    date_format(\n",
    "        to_timestamp(acc_df[\"EndTime(UTC)\"], \"yyyy-MM-dd'T'HH:mm:ss\"), \n",
    "        \"yyyy-MM-dd'T'HH:mm:ss\"\n",
    "    ).alias('timestamp_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d79de82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {'LosAngeles': [33.700615, 34.353627, -118.683511, -118.074559], \n",
    "           'Houston': [29.497907,30.129003,-95.797178,-94.988191],\n",
    "           'Austin': [30.079327, 30.596764,-97.968881,-97.504838],\n",
    "           'Dallas': [32.559567,33.083278,-97.036586,-96.428928],\n",
    "           'Charlotte': [34.970168,35.423667,-81.060925,-80.622687],\n",
    "           'Atlanta': [33.612410,33.916999,-84.575600,-84.231911]}\n",
    "\n",
    "time_zones = {'Houston':'US/Central', 'Charlotte':'US/Eastern', 'Dallas':'US/Central',\n",
    "              'Atlanta':'US/Eastern', 'Austin':'US/Central', 'LosAngeles':'US/Pacific'}\n",
    "\n",
    "# time interval to sample data for \n",
    "start = datetime(2018, 6, 1)\n",
    "finish   = datetime(2018, 9, 2)\n",
    "\n",
    "begin = datetime.strptime('2018-06-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "end   = datetime.strptime('2018-08-31 23:59:59', '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2085b534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[EventId: string, Type: string, Severity: int, TMC: int, Description: string, StartTime(UTC): string, EndTime(UTC): string, TimeZone: string, LocationLat: double, LocationLng: double, Distance(mi): double, AirportCode: string, Number: int, Street: string, Side: string, City: string, County: string, State: string, ZipCode: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[EventId: string, Type: string, Severity: int, TMC: int, Description: string, StartTime(UTC): string, EndTime(UTC): string, TimeZone: string, LocationLat: double, LocationLng: double, Distance(mi): double, AirportCode: string, Number: int, Street: string, Side: string, City: string, County: string, State: string, ZipCode: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[EventId: string, Type: string, Severity: int, TMC: int, Description: string, StartTime(UTC): string, EndTime(UTC): string, TimeZone: string, LocationLat: double, LocationLng: double, Distance(mi): double, AirportCode: string, Number: int, Street: string, Side: string, City: string, County: string, State: string, ZipCode: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[EventId: string, Type: string, Severity: int, TMC: int, Description: string, StartTime(UTC): string, EndTime(UTC): string, TimeZone: string, LocationLat: double, LocationLng: double, Distance(mi): double, AirportCode: string, Number: int, Street: string, Side: string, City: string, County: string, State: string, ZipCode: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[EventId: string, Type: string, Severity: int, TMC: int, Description: string, StartTime(UTC): string, EndTime(UTC): string, TimeZone: string, LocationLat: double, LocationLng: double, Distance(mi): double, AirportCode: string, Number: int, Street: string, Side: string, City: string, County: string, State: string, ZipCode: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[EventId: string, Type: string, Severity: int, TMC: int, Description: string, StartTime(UTC): string, EndTime(UTC): string, TimeZone: string, LocationLat: double, LocationLng: double, Distance(mi): double, AirportCode: string, Number: int, Street: string, Side: string, City: string, County: string, State: string, ZipCode: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for c in cities:\n",
    "    crds = cities[c]\n",
    "    subset_all = acc_df.where((acc_df['StartTime(UTC)'] >= start) & (acc_df['StartTime(UTC)'] < end) \\\n",
    "                             & (acc_df['LocationLat'] > crds[0]) & (acc_df['LocationLat'] < crds[1]) \\\n",
    "                             & (acc_df['LocationLng'] > crds[2]) & (acc_df['LocationLng'] < crds[3]))\n",
    "    \n",
    "    \n",
    "    subset_accidents = acc_df.where((acc_df['Type']=='Accident') & (acc_df['StartTime(UTC)'] >= start) \\\n",
    "                                    & (acc_df['StartTime(UTC)'] < finish) & (acc_df['LocationLat']>crds[0]) \\\n",
    "                                    & (acc_df['LocationLat']<crds[1]) & (acc_df['LocationLng']>crds[2]) \\\n",
    "                                    & (acc_df['LocationLng']<crds[3]))\n",
    "    \n",
    "    print(subset_all)\n",
    "    \n",
    "    subset_all.coalesce(1).write.option(\"header\", \"true\")\\\n",
    "              .option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "              .csv('temporary/MQ_{}_all_time.csv'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d6eab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MQ LosAngeles 92989\n",
      "['T-21832562', 'Broken-Vehicle', 33.777599, -118.09063, datetime.datetime(2018, 8, 30, 21, 39, 48, tzinfo=<DstTzInfo 'US/Pacific' PDT-1 day, 17:00:00 DST>), datetime.datetime(2018, 8, 30, 22, 24, 12, tzinfo=<DstTzInfo 'US/Pacific' PDT-1 day, 17:00:00 DST>)]\n",
      "MQ Houston 45463\n",
      "['T-19839528', 'Construction', 29.696104, -95.293938, datetime.datetime(2018, 7, 30, 12, 39, 44, tzinfo=<DstTzInfo 'US/Central' CDT-1 day, 19:00:00 DST>), datetime.datetime(2018, 12, 31, 15, 0, tzinfo=<DstTzInfo 'US/Central' CST-1 day, 18:00:00 STD>)]\n",
      "MQ Austin 20440\n",
      "['T-17479865', 'Construction', 30.257509, -97.678627, datetime.datetime(2018, 6, 15, 6, 0, tzinfo=<DstTzInfo 'US/Central' CDT-1 day, 19:00:00 DST>), datetime.datetime(2019, 4, 1, 6, 0, tzinfo=<DstTzInfo 'US/Central' CDT-1 day, 19:00:00 DST>)]\n",
      "MQ Dallas 30545\n",
      "['T-8516587', 'Construction', 32.821251, -96.746582, datetime.datetime(2018, 8, 24, 8, 0, tzinfo=<DstTzInfo 'US/Central' CDT-1 day, 19:00:00 DST>), datetime.datetime(2020, 8, 13, 18, 0, tzinfo=<DstTzInfo 'US/Central' CDT-1 day, 19:00:00 DST>)]\n",
      "MQ Charlotte 18600\n",
      "['T-8511825', 'Construction', 35.204948, -80.812576, datetime.datetime(2018, 6, 4, 9, 0, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>), datetime.datetime(2020, 7, 31, 17, 0, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)]\n",
      "MQ Atlanta 23944\n",
      "['T-11294335', 'Construction', 33.752789, -84.393448, datetime.datetime(2018, 8, 1, 5, 0, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>), datetime.datetime(2019, 12, 1, 7, 0, tzinfo=<DstTzInfo 'US/Eastern' EST-1 day, 19:00:00 STD>)]\n"
     ]
    }
   ],
   "source": [
    "path = 'temporary/' #create this temporary directory inside the '/data' folder\n",
    "mq_city2incidents = {}\n",
    "for c in cities:\n",
    "    incidents = []\n",
    "    z = time_zones[c]\n",
    "    \n",
    "    with open(path + 'MQ_{}_all_time.csv'.format(c), 'r', encoding='utf-8') as file:\n",
    "        header = False\n",
    "        for line in file:\n",
    "            if not header:\n",
    "                header = True\n",
    "                continue\n",
    "            parts = line.replace('\\r', '').replace('\\n', '').split(',')\n",
    "            \n",
    "            ds = datetime.strptime(parts[5].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n",
    "            ds = ds.replace(tzinfo=pytz.utc)\n",
    "            ds = ds.astimezone(pytz.timezone(z))\n",
    "            \n",
    "            de = datetime.strptime(parts[6].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n",
    "            de = de.replace(tzinfo=pytz.utc)\n",
    "            de = de.astimezone(pytz.timezone(z))\n",
    "            \n",
    "            v = [parts[0], parts[1], float(parts[8]), float(parts[9]), ds, de]            \n",
    "            incidents.append(v)\n",
    "            \n",
    "    mq_city2incidents[c] = incidents\n",
    "    print ('MQ', c, len(incidents))\n",
    "    print(incidents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3909d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_to_be = {}\n",
    "\n",
    "for z in ['US/Eastern', 'US/Central', 'US/Mountain', 'US/Pacific']:\n",
    "    t_begin = begin.replace(tzinfo=pytz.timezone(z)) # t_begin : 根据utc转换后的开始时间\n",
    "    t_end   = end.replace(tzinfo=pytz.timezone(z)) # t_end: 根据utc转换后的结束时间\n",
    "    zone_to_be[z] = [t_begin, t_end]\n",
    "\n",
    "name_conversion = {'Broken-Vehicle':'BrokenVehicle', 'Flow-Incident': 'FlowIncident', 'Lane-Blocked':'RoadBlocked'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f48af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: 8831\n",
      "Done with LosAngeles in 5.0 sec! there are 163 geohashes with data!\n",
      "Done with Houston in 2.9 sec! there are 249 geohashes with data!\n",
      "Done with Austin in 1.4 sec! there are 130 geohashes with data!\n",
      "Done with Dallas in 2.0 sec! there are 164 geohashes with data!\n",
      "Done with Charlotte in 1.2 sec! there are 114 geohashes with data!\n",
      "Done with Atlanta in 1.3 sec! there are 68 geohashes with data!\n"
     ]
    }
   ],
   "source": [
    "def return_interval_index(time_stamp, start, end):\n",
    "    if time_stamp < start or time_stamp>end: \n",
    "        return -1\n",
    "    index = int(((time_stamp - start).days*24*60 + (time_stamp-start).seconds/60)/15)\n",
    "    return index\n",
    "\n",
    "diff = int(((end - begin).days*24*60 + (end-begin).seconds/60)/15) # total_minutes/15 ==> number of 15 minutes intervals\n",
    "print(f'diff: {diff}')\n",
    "\n",
    "path = 'temporary/'\n",
    "city_to_geohashes = {}\n",
    "for c in cities: city_to_geohashes[c] = {}\n",
    "\n",
    "start_timestamp = time.time()\n",
    "ccnntt = 0\n",
    "\n",
    "geocode_to_airport = {}\n",
    "aiport_to_timezone = {}\n",
    "\n",
    "for c in cities:\n",
    "    z = time_zones[c]\n",
    "    \n",
    "    # add map-quest data\n",
    "    with open(path + 'MQ_{}_all_time.csv'.format(c), 'r', encoding='utf-8') as file:\n",
    "        header = False\n",
    "        for line in file:\n",
    "            if not header:\n",
    "                header = True\n",
    "                continue\n",
    "            parts = line.replace('\\r', '').replace('\\n', '').split(',')\n",
    "            \n",
    "            ds = datetime.strptime(parts[5].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n",
    "            ds = ds.replace(tzinfo=pytz.utc)\n",
    "            ds = ds.astimezone(pytz.timezone(z))\n",
    "            s_interval = return_interval_index(ds, zone_to_be[z][0], zone_to_be[z][1])\n",
    "            if s_interval==-1: continue\n",
    "                \n",
    "            de = datetime.strptime(parts[6].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n",
    "            de = de.replace(tzinfo=pytz.utc)\n",
    "            de = de.astimezone(pytz.timezone(z))\n",
    "            e_interval = return_interval_index(de, zone_to_be[z][0], zone_to_be[z][1])\n",
    "            if e_interval == -1: e_interval = diff-1    \n",
    "            \n",
    "            start_gh = gh.encode(float(parts[8]), float(parts[9]), precision=geohash_prec)\n",
    "            intervals = []\n",
    "            if start_gh not in city_to_geohashes[c]:\n",
    "                for i in range(diff): \n",
    "                    intervals.append({'Construction':0, 'Congestion':0, 'Accident':0, 'FlowIncident':0, 'Event':0, \n",
    "                                      'BrokenVehicle':0, 'RoadBlocked':0, 'Other':0})\n",
    "            else:\n",
    "                intervals = city_to_geohashes[c][start_gh]\n",
    "            \n",
    "            if parts[1] in name_conversion:\n",
    "                tp = name_conversion[parts[1]]\n",
    "            else: \n",
    "                tp = parts[1].split('-')[0]\n",
    "                \n",
    "            for i in range(s_interval, e_interval+1):                \n",
    "                v = intervals[i]\n",
    "                if tp in v: v[tp] = v[tp] + 1\n",
    "                else: v['Other'] = v['Other'] + 1\n",
    "                intervals[i] = v\n",
    "                \n",
    "                if tp == 'Accident': break # unlike other types of traffic events, \n",
    "                \n",
    "            city_to_geohashes[c][start_gh] = intervals\n",
    "            \n",
    "            ap = parts[11]\n",
    "            if len(ap) > 3:\n",
    "                if start_gh not in geocode_to_airport:\n",
    "                    geocode_to_airport[start_gh] = set([ap])\n",
    "                else:\n",
    "                    st = geocode_to_airport[start_gh]\n",
    "                    st.add(ap)\n",
    "                    geocode_to_airport[start_gh] = st\n",
    "                aiport_to_timezone[ap] = z\n",
    "  \n",
    "    \n",
    "    print('Done with {} in {:.1f} sec! there are {} geohashes with data!'.format(c, \n",
    "                                time.time()-start_timestamp, len(city_to_geohashes[c])))\n",
    "    start_timestamp = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b63b12fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 airports to collect data for!\n",
      "Airport KLGB\n",
      "Airport KSLI\n",
      "Airport KLAX\n",
      "Airport KHHR\n",
      "Airport KSMO\n",
      "Airport KVNY\n",
      "Airport KBUR\n",
      "Airport KTOA\n",
      "Airport KCQT\n",
      "Airport KMWS\n",
      "no file for Airport KMWS\n",
      "Airport KFUL\n",
      "Airport KWHP\n",
      "Airport KEMT\n",
      "Airport K3A6\n",
      "Airport KGXA\n",
      "Airport KPMD\n",
      "Airport KHOU\n",
      "Airport KMCJ\n",
      "Airport KSGR\n",
      "Airport KDWH\n",
      "Airport KTME\n",
      "Airport KIAH\n",
      "Airport KEFD\n",
      "Airport KAXH\n",
      "Airport KLVJ\n",
      "Airport K6R3\n",
      "Airport KAUS\n",
      "Airport KATT\n",
      "Airport KRYW\n",
      "Airport KEDC\n",
      "Airport KGTU\n",
      "Airport KHQZ\n",
      "Airport KDAL\n",
      "Airport KDFW\n",
      "Airport KADS\n",
      "Airport KRBD\n",
      "Airport KGPM\n",
      "Airport KLNC\n",
      "Airport KF46\n",
      "Airport KTKI\n",
      "Airport KDTO\n",
      "Airport KGKY\n",
      "Airport KJQF\n",
      "Airport KCLT\n",
      "Airport KEQY\n",
      "Airport KUZA\n",
      "Airport KAKH\n",
      "Airport KIPJ\n",
      "Airport KPDK\n",
      "Airport KATL\n",
      "Airport KFTY\n",
      "Airport KMGE\n",
      "\n",
      "Data for 51 airport stations is loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# load and sort relevant weather data\n",
    "airports_to_observations = {}\n",
    "for g in geocode_to_airport:\n",
    "    aps = geocode_to_airport[g]\n",
    "    for a in aps:\n",
    "        if a not in airports_to_observations:\n",
    "            airports_to_observations[a] = []\n",
    "\n",
    "print(f'{len(airports_to_observations)} airports to collect data for!')\n",
    "            \n",
    "w_path = 'Sample_Weather/' # this directory contains weather observation records for each airport\n",
    "# a sample data file can be find in '/data' directory (Sample_Weather.tar.gz)\n",
    "airport_to_data = {}\n",
    "for ap in airports_to_observations:\n",
    "    data = []\n",
    "    z = aiport_to_timezone[ap]\n",
    "    print(f'Airport {ap}')\n",
    "    header = ''\n",
    "    if not os.path.isfile(w_path + ap + '.csv'):\n",
    "        print(f'no file for Airport {ap}')\n",
    "        continue\n",
    "    with open(w_path + ap + '.csv', 'r') as file:\n",
    "        for line in file:\n",
    "            if 'Airport' in line: \n",
    "                header = line.replace('\\r','').replace('\\n','').replace(',Hour','')\n",
    "                continue\n",
    "            parts = line.replace('\\r', '').replace('\\n', '').split(',')\n",
    "            try:\n",
    "                w = weather(parts[1] + ' ' + parts[2].split(' ')[0] + ':00 ' + parts[2].split(' ')[1], parts[3], parts[4], \n",
    "                           parts[5], parts[6], parts[7], parts[8], parts[9], parts[10], parts[11], parts[12], z)   \n",
    "                data.append(w)\n",
    "            except:\n",
    "                continue\n",
    "    data.sort(key=lambda x:x.date)\n",
    "    airport_to_data[ap] = data\n",
    "    \n",
    "print(f'\\nData for {len(airport_to_data)} airport stations is loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cefb080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in city_to_geohashes:\n",
    "    for g in city_to_geohashes[c]:\n",
    "        if g not in geocode_to_airport:\n",
    "            gc = gh.decode_exactly(g)[0:2]\n",
    "            min_dist = 1000000000\n",
    "            close_g = ''\n",
    "            for _g in geocode_to_airport:\n",
    "                _gc = gh.decode_exactly(_g)[0:2]\n",
    "                dst = haversine(gc, _gc, 'km')\n",
    "                if dst < min_dist:\n",
    "                    min_dist = dst\n",
    "                    close_g = _g\n",
    "#             print g, close_g, min_dist\n",
    "            geocode_to_airport[g] = geocode_to_airport[close_g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "399e61d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LosAngeles in 18.2 sec!\n",
      "Done with Houston in 36.2 sec!\n",
      "Done with Austin in 57.6 sec!\n",
      "Done with Dallas in 44.3 sec!\n",
      "Done with Charlotte in 6.1 sec!\n",
      "Done with Atlanta in 45.3 sec!\n"
     ]
    }
   ],
   "source": [
    "city_to_geohashes_to_weather = {}\n",
    "\n",
    "for c in city_to_geohashes:\n",
    "    start = time.time()\n",
    "    geo2weather = {}\n",
    "    for g in city_to_geohashes[c]:\n",
    "        w_data = []\n",
    "        for i in range(len(city_to_geohashes[c][g])):\n",
    "            w_data.append({'Temperature':[], 'Humidity':[], 'Pressure':[], 'Visibility':[], 'WindSpeed':[], \n",
    "                          'Precipitation':[], 'Condition':set(), 'Event':set()})\n",
    "        # populate weather data\n",
    "        aps = geocode_to_airport[g]\n",
    "        for a in aps:\n",
    "            z = aiport_to_timezone[a]\n",
    "            if a not in airport_to_data:\n",
    "                continue\n",
    "            a_w_data = airport_to_data[a]\n",
    "            prev = 0\n",
    "            for a_w_d in a_w_data:\n",
    "                idx = return_interval_index(a_w_d.date, zone_to_be[z][0], zone_to_be[z][1])\n",
    "                if idx >-1:\n",
    "                    for i in range(prev, min(idx+1, len(w_data))):\n",
    "                        _w = w_data[i]\n",
    "                        \n",
    "                        _tmp = _w['Temperature']\n",
    "                        if a_w_d.temp > -1000:\n",
    "                            _tmp.append(a_w_d.temp)\n",
    "                            _w['Temperature'] = _tmp\n",
    "                        \n",
    "                        _hmd = _w['Humidity']\n",
    "                        if a_w_d.humid > -1000:\n",
    "                            _hmd.append(a_w_d.humid)\n",
    "                            _w['Humidity'] = _hmd\n",
    "                        \n",
    "                        _prs = _w['Pressure']\n",
    "                        if a_w_d.pressure > -1000:\n",
    "                            _prs.append(a_w_d.pressure)\n",
    "                            _w['Pressure'] = _prs\n",
    "                        \n",
    "                        _vis = _w['Visibility']\n",
    "                        if a_w_d.visib > -1000:\n",
    "                            _vis.append(a_w_d.visib)\n",
    "                            _w['Visibility'] = _vis\n",
    "                            \n",
    "                        _wspd = _w['WindSpeed']\n",
    "                        if a_w_d.windspeed > -1000:\n",
    "                            _wspd.append(a_w_d.windspeed)\n",
    "                            _w['WindSpeed'] = _wspd\n",
    "                            \n",
    "                        _precip = _w['Precipitation']\n",
    "                        if a_w_d.precipitation > -1000:\n",
    "                            _precip.append(a_w_d.precipitation)\n",
    "                            _w['Precipitation'] = _precip\n",
    "                            \n",
    "                        _cond = _w['Condition']\n",
    "                        _cond.add(a_w_d.condition)\n",
    "                        _w['Condition'] = _cond\n",
    "                        \n",
    "                        _evnt = _w['Event']\n",
    "                        _evnt.add(a_w_d.events)\n",
    "                        _w['Event'] = _evnt\n",
    "                        \n",
    "                        w_data[i] = _w\n",
    "                        \n",
    "                    prev = idx+1\n",
    "                                                \n",
    "            \n",
    "        geo2weather[g] = w_data\n",
    "    city_to_geohashes_to_weather[c] = geo2weather\n",
    "    print('Done with {} in {:.1f} sec!'.format(c, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d56e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dayLight:\n",
    "    sunrise = []\n",
    "    sunset = []\n",
    "    def __init__(self, sunrise, sunset):\n",
    "        self.sunrise = sunrise\n",
    "        self.sunset = sunset\n",
    "        \n",
    "def return_time(x):\n",
    "    try:\n",
    "        h = int(x.split(':')[0])\n",
    "        m = int(x.split(':')[1].split(' ')[0])\n",
    "        if 'pm' in x and h < 12: h = h + 12\n",
    "        return [h,m]\n",
    "    except: return [0,0]\n",
    "\n",
    "    \n",
    "def returnDayLight(city, state, dt):\n",
    "    sc = city + '-' + state\n",
    "    days = city_days_time[sc]\n",
    "    d = str(dt.year) + '-' + str(dt.month) + '-' + str(dt.day)\n",
    "    if d in days:\n",
    "        r = days[d]\n",
    "        if ((dt.hour>r.sunrise[0] and dt.hour<r.sunset[0]) or\n",
    "            (dt.hour>=r.sunrise[0] and dt.minute>=r.sunrise[1] and dt.hour<r.sunset[0]) or\n",
    "            (dt.hour>r.sunrise[0] and dt.hour<=r.sunset[0] and dt.minute<r.sunset[1]) or \n",
    "            (dt.hour>=r.sunrise[0] and dt.minute>=r.sunrise[1] and dt.hour<=r.sunset[0] and dt.minute<r.sunset[1])):\n",
    "            return '1'\n",
    "        else: return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda71818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded daylight data for 6 cities!\n"
     ]
    }
   ],
   "source": [
    "city_days_time = {}\n",
    "\n",
    "days = {}\n",
    "city = ''\n",
    "with open('sample_daylight.csv', 'r') as file: # you find daylight data for the selected 6 cities in this file\n",
    "    for ln in file.readlines():\n",
    "        parts = ln.replace('\\r','').replace('\\n','').split(',')\n",
    "\n",
    "        if parts[0] != city:\n",
    "            if len(city) > 0: \n",
    "                if city in city_days_time:\n",
    "                    _days = city_days_time[city]\n",
    "                    for _d in _days: days[_d] = _days[_d]\n",
    "                city_days_time[city] = days\n",
    "\n",
    "            city = parts[0]\n",
    "            days = {}\n",
    "\n",
    "        sunrise = return_time(parts[2])\n",
    "        sunset  = return_time(parts[3])\n",
    "        dl = dayLight(sunrise, sunset)\n",
    "        days[parts[1]] = dl\n",
    "\n",
    "if city in city_days_time:\n",
    "    _days = city_days_time[city]\n",
    "    for _d in _days: days[_d] = _days[_d]\n",
    "city_days_time[city] = days\n",
    "\n",
    "\n",
    "print('Successfully loaded daylight data for {} cities!'.format(len(city_days_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cb0b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-load daylight mapping for different cities\n",
    "city_to_index_to_daylight = {}\n",
    "states = {'Houston':'TX', 'Charlotte':'NC', 'Dallas':'TX', 'Atlanta':'GA', 'Austin':'TX', 'LosAngeles':'CA'}\n",
    "for c in cities:\n",
    "    d_begin = begin.replace(tzinfo=pytz.timezone(time_zones[c]))\n",
    "    d_end   = end.replace(tzinfo=pytz.timezone(time_zones[c]))\n",
    "    index_to_daylight = {}\n",
    "    index = 0\n",
    "    while(d_begin < d_end):\n",
    "        dl = returnDayLight(c, states[c], d_begin)\n",
    "        index_to_daylight[index] = dl\n",
    "        index += 1\n",
    "        d_begin += timedelta(seconds=15*60)\n",
    "    city_to_index_to_daylight[c] = index_to_daylight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86deac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each time-step to hour of day and day of the week; this should be consistent across different time-zones!\n",
    "timestep_to_dow_hod = {}\n",
    "d_begin = begin.replace(tzinfo=pytz.utc)\n",
    "d_end   = end.replace(tzinfo=pytz.utc)\n",
    "index = 0\n",
    "\n",
    "while(d_begin < d_end):\n",
    "    dow = d_begin.weekday()\n",
    "    hod = d_begin.hour    \n",
    "    timestep_to_dow_hod[index] = [dow, hod]\n",
    "    \n",
    "    d_begin += timedelta(seconds=15*60)    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ceb7d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with LosAngeles in 63.0 sec! #vectors 1439453!\n",
      "Done with Houston in 98.9 sec! #vectors 2198919!\n",
      "Done with Austin in 52.7 sec! #vectors 1148030!\n",
      "Done with Dallas in 66.4 sec! #vectors 1448284!\n",
      "Done with Charlotte in 45.1 sec! #vectors 1006734!\n",
      "Done with Atlanta in 27.1 sec! #vectors 600508!\n"
     ]
    }
   ],
   "source": [
    "traffic_tags = ['Accident', 'BrokenVehicle', 'Congestion', 'Construction', 'Event', 'FlowIncident', 'Other', 'RoadBlocked']\n",
    "weather_tags = ['Condition', 'Event', 'Humidity', 'Precipitation', 'Pressure', 'Temperature', 'Visibility', 'WindSpeed']\n",
    "poi_tags = []\n",
    "start = time.time()\n",
    "condition_tags = set()\n",
    "\n",
    "for c in city_to_geohashes:\n",
    "    # creating vector for each reion (geohash) during a 15 minutes time interval. Such vector contains time, traffic, and weather attributes. \n",
    "    writer = open('{}_geo2vec_{}-{}.csv'.format(c, str(begin.year)+str(begin.month)+str(begin.day),\n",
    "                                                        str(end.year)+str(end.month)+str(end.day)), 'w')\n",
    "    writer.write('Geohash,TimeStep,DOW,HOD,DayLight,T-Accident,T-BrokenVehicle,T-Congestion,T-Construction,'\\\n",
    "        'T-Event,T-FlowIncident,T-Other,T-RoadBlocked,W-Humidity,W-Precipitation,W-Pressure,'\\\n",
    "        'W-Temperature,W-Visibility,W-WindSpeed,W-Rain,W-Snow,W-Fog,W-Hail\\n')\n",
    "    \n",
    "    traffic = city_to_geohashes[c]\n",
    "    weather = city_to_geohashes_to_weather[c]        \n",
    "    for g in traffic:\n",
    "        vectors = []\n",
    "        for i in range(len(traffic[g])):\n",
    "            v = []\n",
    "            for t in traffic_tags: v.append(traffic[g][i][t])\n",
    "            v_w = [0,0,0,0] # for rain, snow, fog, and hail\n",
    "            for w in weather_tags:\n",
    "                if w=='Condition' or w=='Event':      \n",
    "                    _tgs = weather[g][i][w]\n",
    "                    for _tg in _tgs: \n",
    "                        if 'rain' in _tg.lower() or 'drizzle' in _tg.lower() or 'thunderstorm' in _tg.lower(): v_w[0] = 1\n",
    "                        elif 'snow' in _tg.lower(): v_w[1] = 1\n",
    "                        elif 'fog' in _tg.lower() or 'haze' in _tg.lower() or 'mist' in _tg.lower() or 'smoke' in _tg.lower(): v_w[2] = 1\n",
    "                        elif 'hail' in _tg.lower() or 'ice pellets' in _tg.lower(): v_w[3] = 1                            \n",
    "                elif len(weather[g][i][w]) == 0: v.append(0)\n",
    "                else: v.append(np.mean(weather[g][i][w]))\n",
    "            for _v_w in v_w: v.append(_v_w)\n",
    "            vectors.append(v)\n",
    "        \n",
    "        for i in range(len(vectors)):\n",
    "            v = vectors[i]\n",
    "            v = [str(v[j]) for j in range(len(v))]\n",
    "            v = ','.join(v)\n",
    "            writer.write(g + ',' + str(i) + ',' + str(timestep_to_dow_hod[i][0]) + ',' + str(timestep_to_dow_hod[i][1]) \n",
    "                         + ',' + city_to_index_to_daylight[c][i] + ',' + v + '\\n')\n",
    "            \n",
    "    writer.close()\n",
    "    print('Done with {} in {:.1f} sec! #vectors {}!'.format(c, time.time()-start, len(traffic)*len(vectors)))\n",
    "    start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee69ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
